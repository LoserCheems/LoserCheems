# Jingze Shi

<!-- I prefer something with more practical value rather than just a story. -->

<!--  **news**: I am looking for a engineering internship in the field of LLM. If you have any information, don't hesitate to get in touch with me. üìß -->

### *Experience* üêï

- 2022.9-*Present* Undergraduate Student
<!-- - 2025.10-2026.2 Research Assistant of The Hong Kong University of Science and Technology -->


## Competition Awards üèÜ

- 2023.11-2023.12 **[Hundreds Models to Thousands Modals](https://competition.huaweicloud.com/information/1000041979/introduction)** First Prize
- 2024.06-2024.11 **[Huawei Track of Challenge Cup](https://competition.huaweicloud.com/information/1000042047/introduction)** First Prize
- 2025.06-2025.09 **[PAZHOU Algorithm Competition](https://deepvision.aicompetition-pz.com/#/homeDetail?id=1933438078467272705)** First Prize


## Publications üìù

- **Trainable Dynamic Mask Sparse Attention** [[Paper](https://arxiv.org/abs/2508.02124)]
- **OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale** [[Paper](https://arxiv.org/abs/2602.05711)]
- **Towards Automated Kernel Generation in the Era of LLMs** [[Paper](https://arxiv.org/abs/2601.15727)]
- **Concise Reasoning, Big Gains: Pruning Long Reasoning Trace with Difficulty-Aware Prompting** [[Paper](https://arxiv.org/abs/2505.19716)]


## Research Direction üî≠

- Natural Language Processing
- Large Language Models
- Small Language Models
- Foundation Models
- Deep Reinforcement Learning
- High Efficient Algorithm


## Skills ‚öíÔ∏è

- Natural Language: ÁÆÄ‰Ωì‰∏≠Êñá, English
- Programming Language: **C++**, Python
- Typesetting Language: **Markdown**, LaTeX
- Programming Framework: **PyTorch**, Transformers


